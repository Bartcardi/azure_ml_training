{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Bartcardi/azure_ml_training/blob/duckdb_talk/notebooks/duckdb_talk.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "# Setup kaggle credentials\n",
    "\n",
    " Visit www.kaggle.com. Go to your profile and click on account. On the following page you will see an API section, where you will find a “Create New API Token” click on it, and it will download a kaggle.json file in which you will get your username and key. \n",
    "\n",
    "# If on Colab\n",
    "\n",
    "Follow the instructions found here: [https://www.kaggle.com/discussions/general/74235#2580958](https://www.kaggle.com/discussions/general/74235#2580958) to set Colab secrets found in the downloaded `kaggle.json` file. When the secrets are added and set to notebook access run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "# If *not* on Colab\n",
    "\n",
    "Copy the downloaded `kaggle.json` to `~/.kaggle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d zanjibar/100-million-data-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip 100-million-data-csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm 100-million-data-csv.zip custom_1988_2020.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "user_expressions": [
     {
      "expression": "now",
      "result": {
       "ename": "NameError",
       "evalue": "name 'now' is not defined",
       "status": "error",
       "traceback": [
        "\u001b[0;31mNameError\u001b[0m\u001b[0;31m:\u001b[0m name 'now' is not defined\n"
       ]
      }
     }
    ]
   },
   "source": [
    "# \"DuckDB: Your New Favorite Analytical Tool\"\n",
    "\n",
    "* Bart Joosten / ilionx\n",
    "* 12-02-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# The Story of DuckDB (Origin)\n",
    "\n",
    "* DuckDB was born out of academic research at CWI (Centrum Wiskunde & Informatica) in the Netherlands.\n",
    "* Originally developed by Mark Raasveldt and Hannes Mühleisen.\n",
    "* The initial goal was to create an in-process database system optimized for analytical queries on embedded devices.\n",
    "* The project quickly evolved into a powerful and versatile analytical DBMS suitable for a wide range of applications.\n",
    "* It's open-source (MIT License) and has a vibrant community contributing to its development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What is DuckDB?\n",
    "\n",
    "* DuckDB is an embedded or  analytical database management system (DBMS).\n",
    "* It's designed to be fast, portable, and easy to use, especially for analytical queries.\n",
    "* Written in C++ with zero dependies (only working C++11 compiler required).\n",
    "* Think of it as SQLite for analytics.\n",
    "* **Key Feature:** Optimized for analytical workloads (OLAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Why Use DuckDB (Especially with Python)?\n",
    "\n",
    "* **Speed:** DuckDB is *significantly* faster than using Pandas or other Python libraries for many analytical operations, especially on larger datasets.  It pushes computation down into the database engine, which is highly optimized.\n",
    "* **Ease of Use:**  It's embedded! No setting up a separate database server. Just install the `duckdb` Python package and you're ready to go.\n",
    "* **SQL Power:** Leverage the full power of SQL for complex queries, aggregations, and data transformations directly within your Python code. This can be more concise and efficient than equivalent Pandas code.\n",
    "* **Seamless Integration:** The `duckdb` Python library provides a smooth interface for interacting with DuckDB. You can easily load data from Pandas DataFrames, execute SQL queries, and retrieve results back into Pandas.\n",
    "* **Portability and Reproducibility:**  DuckDB is entirely self-contained.  This makes your analyses portable (serverless) and reproducible.  You can easily share your code and data without worrying about database configurations.\n",
    "* **Parquet and other formats:** Read and write Parquet, CSV, and other data science-friendly formats directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# OLAP vs. OLTP - The Core Difference\n",
    "\n",
    "* **OLTP (Online Transaction Processing):**\n",
    "    * Designed for transactional workloads. Think of your online banking system or e-commerce checkout.\n",
    "    * Focus: High volume of small transactions, data consistency, and speed of individual transactions.\n",
    "    * Examples: Inserting a new customer, updating an order status.\n",
    "* **OLAP (Online Analytical Processing):**\n",
    "    * Designed for complex analytical queries. Think of business intelligence dashboards or data science exploration.\n",
    "    * Focus: Analyzing large datasets, complex aggregations, and query performance.\n",
    "    * Examples: Calculating sales trends over time, identifying customer segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# OLAP vs. OLTP - Comparison Table\n",
    "\n",
    "| Feature           | OLTP                               | OLAP                                  |\n",
    "|-------------------|------------------------------------|---------------------------------------|\n",
    "| Workload          | Transactions (inserts, updates)      | Analytical queries (SELECTs, aggregations) |\n",
    "| Data Volume       | Relatively small transactions         | Large datasets                           |\n",
    "| Query Complexity  | Simple, fast queries                 | Complex queries, aggregations             |\n",
    "| Performance Goal  | Transaction speed, data consistency  | Query speed, data analysis              |\n",
    "| Data Changes      | Frequent, small updates              | Infrequent, bulk updates                |\n",
    "| Data Focus        | Current, operational data            | Historical, analytical data             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Why DuckDB is OLAP-focused\n",
    "\n",
    "* DuckDB's architecture and optimizations are specifically geared towards OLAP workloads.\n",
    "* **Columnar storage:** Data is stored column-wise, which is much more efficient for analytical queries that often only access a subset of columns. *(Reduced I/O, better compression)*\n",
    "* **Vectorized query execution:** DuckDB processes data in batches (vectors), leading to significant performance gains.\n",
    "* **Optimized query planner:** DuckDB's query planner is designed to find the most efficient execution plan for complex analytical queries.\n",
    "* These features make DuckDB significantly faster than a traditional row-oriented database (like SQLite) for analytical tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# DuckDB Use Cases for Data Scientists\n",
    "\n",
    "* **Local data analysis:** Analyze large datasets on your laptop without setting up a complex database server.\n",
    "* **Data exploration and prototyping:** Quickly test out different analytical queries and transformations.\n",
    "* **Reproducible research:** Embed DuckDB directly into your analysis scripts to ensure reproducibility.\n",
    "* **Integration with data science tools:** Seamlessly use DuckDB with Python (via the `duckdb` library), R, and other languages.\n",
    "* **Parquet and CSV support:** Easily import and export data in common data science formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Demo Time!\n",
    "\n",
    "* Let's see DuckDB in action! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the 100 million rows data in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df_csv = pd.read_csv('custom_1988_2020.csv')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the same data into duckdb transient database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "start = time.time()\n",
    "con.execute(\"CREATE TABLE custom_1988_2020 AS SELECT * FROM 'custom_1988_2020.csv'\")\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a pandas/polars dataframe from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df_pandas = con.execute(\"SELECT * FROM custom_1988_2020\").fetchdf()\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df_polars = con.execute(\"SELECT * FROM custom_1988_2020\").pl()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running sql directly on pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': [1, 2, 3], 'b': ['x', 'y', 'z']})\n",
    "con.register(\"my_pandas_table\", df) # Register a Pandas DataFrame as a virtual table\n",
    "con.execute(\"SELECT * FROM my_pandas_table\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the table to a persistent db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"ATTACH 'persistent.db' AS persistent\").fetchall()\n",
    "con.execute(\"CREATE TABLE persistent.custom_1988_2020 AS SELECT * FROM custom_1988_2020\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening from the persistent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect('persistent.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"DESCRIBE SELECT * FROM custom_1988_2020\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describing statistics in duckdb and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "con.execute(\"SUMMARIZE SELECT * FROM custom_1988_2020\").df()\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Power for Data Exploration\n",
    "\n",
    "Aggregations: Calculate summary statistics (mean, median, standard deviation, etc.) for various columns. Group by different criteria. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SELECT column1, AVG(column2), COUNT(*) FROM custom_1988_2020 GROUP BY column1 ORDER BY COUNT(*) DESC;\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering: Select specific subsets of data based on conditions. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SELECT column4, COUNT(column4) FROM custom_1988_2020 GROUP BY column4 ORDER BY COUNT(column4) DESC;\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SELECT * FROM custom_1988_2020 WHERE column3 > 150 AND column4 = '870829000';\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window Functions: Demonstrate powerful analytical functions like running totals, moving averages, or ranking. Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SELECT column1, column2, ROW_NUMBER() OVER (ORDER BY column2 DESC) as rank FROM custom_1988_2020 LIMIT 10000;\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuckDB as Unix tool\n",
    "\n",
    "https://duckdb.org/2024/06/20/cli-data-processing-using-duckdb-as-a-unix-tool.html\n",
    "\n",
    "## Datasets\n",
    "\n",
    "We use the four input files capturing information on cities and airports in the Netherlands.\n",
    "\n",
    "pop.csv, the population of each of the top-10 most populous cities.\n",
    "\n",
    "area.csv, the area of each of the top-10 most populous cities.\n",
    "\n",
    "cities-airports.csv, the IATA codes of civilian airports serving given cities.\n",
    "\n",
    "airport-names.csv, the airport names belonging to given IATA codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://duckdb.org/data/cli/duckdb-cli-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip duckdb-cli-data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting Columns\n",
    "\n",
    "Projecting columns is a very common data processing step. Let's take the pop.csv file and project the first and last columns, city and population.\n",
    "\n",
    "## Unix Shell: cut\n",
    "In the Unix shell, we use the cut command and specify the file's delimiter (-d) and the columns to be projected (-f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cut -d , -f 1,3 pop.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB: SELECT\n",
    "\n",
    "In DuckDB, we can use the CSV reader to load the data, then use the SELECT clause with column indexes (#i) to designate the columns to be projected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SELECT #1, #3 FROM 'pop.csv';\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "COPY (\n",
    "    SELECT #1, #3 FROM 'pop.csv'\n",
    "  ) TO '/dev/stdout/';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB: POSITIONAL JOIN\n",
    "\n",
    "In DuckDB, we can use a POSITIONAL JOIN. This join type is one of DuckDB's SQL extensions and it provides a concise syntax to combine tables row-by-row based on each row's position in the table. Joining the two tables together using POSITIONAL JOIN results in two city columns – we use the EXCLUDE clause to remove the duplicate column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"\"\"\n",
    "COPY (\n",
    "    SELECT pop.*, area.* EXCLUDE city\n",
    "    FROM 'pop.csv'\n",
    "    POSITIONAL JOIN 'area.csv'\n",
    "  ) TO '/dev/stdout/';\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
